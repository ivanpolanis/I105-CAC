\section{Memoria}

\subsection{Diseño de una jerarquía de memoria básica}

El sistema de memoria formado por una memoria principal única que se propuso en los primeros modelos de computador sencillo quedó descartado rápidamente por sus bajas prestaciones.

La alternativa a esta memoria única es una jerarquía de memoria organizada en niveles, cuanto más cercanos al procesador, más pequeños, rápidos y caros. Hoy en día esta jerarquía incluye, en casi todos los casos, tres tipos de memorias: memoria caché, memoria principal y memoria virtual.

\begin{itemize}
  \item \textbf{Memoria Caché (MC):} Se ubica dentro del mismo chip que el procesador, fabricada con \textit{RAM} estática (SRAM) y controlada por el controlador de caché incluido en el mismo chip. Hoy en día lo habitual es que haya varios niveles de caché.
  \item \textbf{Memoria Principal (MP):} Se ubica en un chip diferente al procesador, fabricada con RAM dinámica (DRAM) y controlada por el controlador de memoria principal. Este controlador es muy importante para el rendimiento de la jerarquía de memoria ya que se encarga de la planificación de los accesos a memoria principal. Hoy en día puede ubicarse en el mismo chip que el procesador y la memoria caché, o en otro chip como el el chipset norte o el MCH.\@
  \item \textbf{Memoria virtual (MV):} Ubicada en la actualidad en el disco duro, se fabrica por lo tanto con tecnología magnética y se controla desde el sistema operativo a través del controlador del disco duro.
\end{itemize}

El objetivo es conseguir una estructura de memoria de gran capacidad, con un coste casi tan bajo como el del nivel más barato de la jerarquía, pero con latencia comparable a la del nivel más rápido.

Hay dos propiedades que la jerarquía debe cumplir para que su funcionamiento sea adecuado:

\begin{itemize}
  \item \textbf{Inclusión:} implica que cualquier información contenida en un nivel de la jerarquía debe estar también en los niveles superiores (en los que están más lejos del procesador a partir de él):
  \item \textbf{Coherencia:} garantiza que las copias de la misma información en los diferentes niveles de la jerarquía son coherentes entre sí, es decir, que almacenan los mismos valores.
\end{itemize}

Cuando el procesador realiza un acceso a memoria, primero se busca la palabra que hay que leer o escribir en la memoria caché \textbf{(MC)}. Si esta palabra se encuentra en la caché, ha ocurrido un acierto. Si por el contrario, no se encuentra a ocurrido un fallo.

En este último caso, se traerá de memoria principal \textbf{(MP)} un bloque que contendrá varias palabras, entre ellas, la que ha producido el fallo.

El \textbf{principio de localidad} tiene dos aspectos diferentes:

\begin{itemize}
  \item \textbf{Localidad espacial:} es la tendencia del procesador a referenciar elementos de memoria cercanos a los últimos que han sido referenciados.
  \item \textbf{Localidad temporal:} es la tendencia del procesador a referenciar elementos de memoria que han sido referenciados recientemente.
\end{itemize}

Al acceder a memoria principal también pueden producirse aciertos y fallos, ya que no toda la información cabe al mismo tiempo en este nivel. La relación entre la memoria principal y la memoria virtual es similar a la que existe entre la memoria caché y la principal.

La memoria principal se divide en \textbf{páginas o segmentos}, de mayor tamaño que los bloques de caché. Cuando se produce un fallo con una página o segmento que no se encuentra en la memoria principal, deberá traerse de la memoria virtual.

La principal diferencia con los fallos de la memoria caché está en que la penalización por este tipo de fallos es bastante grande, ya que la memoria virtual es la más lenta de toda la jerarquía. Por ello el procesador suele pasar a  realizar otro tipo de tareas, realizando un cambio de contexto, hasta que la página o segmento esté disponible en la memoria principal.

\subsection{Mecanismo completo de acceso a memoria}

En primer lugar, el procesador genera la dirección virtual de la palabra que se debe leer o escribir. Normalmente, lo primero que se hace es traducir esta dirección virtual a una dirección física comprensible para la jerarquía de memoria.

Si se consigue realizar esta traducción de dirección, es porque la palabra que está buscando se encuentra actualmente en la memoria principal. Con esta dirección se accede a la memoria caché, y si la palabra buscada se encuentra en este nivel, el acceso a memoria ya ha finalizado.

Si por el contrario, se produce un fallo, se busca la palabra en el siguiente nivel, en este caso en la memoria principal, (si existiese otro nivel de cache, se buscaría primero allí).

La resolución de un fallo siempre implica buscar el bloque necesario en la memoria principal.

Cuando se realice el acceso, todo el bloque en el que se incluye la palabra solicitada por el procesador se envía a la memoria caché para resolver su fallo, y el acceso puede completarse con éxito.

Si por el contrario, la palabra no se encuentra en memoria principal y desde un principio no se pudo traducir su dirección virtual a dirección física por este motivo, se debe resolver el fallo de página o segmento desde la memoria virtual. El sistema operativo realiza un cambio de contexto para que otro proceso pase a ejecutarse en el procesador mientras se trae desde memoria virtual la página o segmento, que hace falta para resolver el fallo. Una vez que esté en memoria principal esta página o segmento, ya puede llevarse el bloque correspondiente a memoria caché y cuando el proceso que había provocado el fallo vuelva a pasar a ejecución, el acceso puede completarse con éxito.

Para un procesador segmentado, los accesos a memoria en la etapa \textbf{MEM}, en su mayoría, sólo tienen en cuenta los accesos a caché, y si hay que resolver un fallo, el tiempo que se tarda en resolver este fallo se considera un tiempo extra que hay que sumar al tiempo que tarda en ejecutarse una instrucción. 

\subsection{Evaluación de prestaciones de la jerarquía de memoria}

En el caso de la jerarquía de memoria también se puede definir una ecuación de prestaciones que proporciones una herramienta cuantitativa de evaluación de rendimiento.

En este caso lo que interesa saber es cuánto tiempo le cuesta en media al procesador realizar un acceso a memoria. Si la memoria caché fuera perfecta y no fallara nunca, el tiempo medio de acceso a memoria sería justo el tiempo de acierto a la memoria caché. Pero como se producen fallos, el tiempo medio de acceso a memoria se calcula teniendo en cuenta estos fallos y el tiempo que se invierte en resolverlos, lo que se ha denominado penalización por fallo:

\begin{align*}
  \centering
  t_{MEM} = t_{acierto MC} + TF \cdot pF 
\end{align*}

\begin{align*}
  t_{acierto MC}&\text{: Tiempo de acierto de MC} \\
  TF&\text{: Tasa de fallos de MC} \to\ TF = \frac{\text{número de fallos}}{\text{número total de accesos a memoria}} \\
  pF&\text{: Penalización por fallo en MC}
\end{align*}

Si lo que interesa es evaluar las prestaciones de la memoria principal o de la memoria virtual como niveles aislados de la jerarquía, las métricas que se suelen utilizarse son:

\begin{itemize}
  \item \textbf{Latencia:} tiempo que transcurre desde que un acceso a memoria comienza hasta que finaliza. Está muy relacionada con la tecnología con la que está fabricada la memoria.
  \item \textbf{Ancho de banda:} cantidad de información por unidad de tiempo que puede transferirse desde/hacia la memoria. En este caso está muy relacionado con la organización de la memoria más que con la tecnología.
\end{itemize}


\subsection{Niveles de la jerarquía de memoria}

\subsubsection{Diseño de la memoria caché}

La memoria caché almacena en cada momento unos determinados bloques de información, por lo tanto se divide en marcos capaces de albergar estos bloques en su interior.

La caché no sólo se compone de marcos, ya que para determinar qué bloque está ocupando un determinado marco en un instante concreto se utilizan etiquetas que también deben almacenarse en la memoria. Esta etiquetas se comparan con la del bloque que se está buscando para determinar si éste se encuentra o no en la memoria caché.

\subsubsection*{Organización de la memoria caché}

El primer aspecto importante a decidir es el tamaño de la cache, no puede ser demasiado pequeña ya que aumentaría el número de fallos porque no se capturaría bien la localidad. 

Tampoco puede ser demasiado grande por dos motivos, el primero es que va integrada en el chip de la cpu y el segundo es que una cache muy grande requeriría una lógica más compleja, por lo que sería más lenta.

El segundo aspecto a tener en cuenta es el tamaño de marco que se va a manejar. Devuelta tenemos el mismo problema, si capturamos bloques muy grandes se captura mejor la localidad espacial, pero a su vez aumenta la penalización por fallo, ya que se necesitaría más tiempo para traer los bloques del siguiente nivel de la jerarquía de memoria.

En muchos casos, la segmentación del procesador obligará a dividir la caché de instrucciones y la de datos para evitar los riesgos estructurales entre las etapas de búsqueda de instrucción y las de acceso a memoria para la lectura o escritura de operandos/resultados.

Por último, destacar dentro de la organización de la caché uno de los aspectos que más influye en el rendimiento de la jerarquía de memoria: la implementación de cachés multinivel.

Normalmente se utilizan dos niveles de memoria caché:

\begin{itemize}
  \item \textbf{L1:} este nivel es el más cercano al procesador, por lo tanto, esta memoria caché es pequeña y rápida.
  \item \textbf{L2:} es el siguiente nivel de la jerarquía, de mayor tamaño (por lo que aprovecha mejor el principio de localidad) y por lo tanto, más lento aunque con menos fallos de capacidad.
\end{itemize}

De esta manera, cuando se produzca un fallo en el nivel 1 de la memoria, la penalización de esta fallo será menor que si sólo hubieran un nivel de caché, porque en lugar de ir a la memoria principal irá a la caché de nivel 2.

\subsubsection*{Política de ubicación}

Hay tres tipos de ubicación:

\begin{itemize}
  \item \textbf{Directo:} a cada bloque le corresponde un único marco de caché y sólo puede alojarse en este marco.
  \item \textbf{Asociativo:} un bloque puede alojarse en cualquier marco de la memoria caché.
  \item \textbf{Asociativo por conjuntos:} la memoria caché se divide en conjuntos con un número determinado de marcos por conjunto. A un determinado bloque le corresponde un único conjunto, pero dentro de él, puede alojarse en cualquier marco.
\end{itemize}

Las diferentes alternativas para la ubicación en la memoria caché llevan a diferentes interpretaciones de la dirección física desde el punto de vista de esta memoria. Según el tipo de ubicación de la memoria caché:

\begin{itemize}
  \item \textbf{Directo:} el índice indica al marco que le corresponde a ese bloque de memoria.
  \item \textbf{Asociativo:} no existe este campo en la dirección, ya que el bloque puede alojarse en cualquier marco.
  \item \textbf{Asociativo por conjuntos:} el índice indica el conjunto que le corresponde a ese bloque de memoria.
\end{itemize}

\subsubsection*{Política de reemplazo}

Además de decidir la política de ubicación de la memoria caché, también es necesario definir qué ocurre cuando se produce un fallo. Si no se encuentra un determinado bloque de memoria en la caché, habrá que traerlo del siguiente nivel de la jerarquía de memoria. Pero, ¿qué bloque se reemplaza?

En el caso de una caché directa, no cabe ninguna duda, puesto que cada bloque sólo puede alojarse en un determinado marco. Pero en el caso de memorias con una asociatividad mayor, el reemplazamiento puede hacerse con distintos tipos de políticas.

\begin{itemize}
  \item \textbf{Aleatoria:} se utiliza un generador de números aleatorios para escoger el bloque que se reemplaza.
  \item \textbf{Least Recently Used (LRU):} se escoge el bloque que lleva más tiempo sin utilizarse. Asi se minimiza la probabilidad de sustituir un bloque que vaya a necesitarse en el futuro.
  \item \textbf{First In, First out (FIFO):} se reemplaza el bloque que lleva más tiempo en la caché.
  \item \textbf{Least Frequently Used (LFU):} esta política reemplaza el bloque que ha sido accedido menos veces en un período de tiempo determinado, es decir, a aquel que se accedió una menor cantidad de veces. Requiere controles de uso (hardware que lleve el conteo de cuantas veces se accedió a un dato de un bloque).
\end{itemize}

\subsubsection*{Política de escritura}

Las escrituras llevan más tiempo ya que no pueden solaparse con la comparación de las etiquetas como se hace con las lecturas. En muchos casos se recupera la información de un marco determinado de caché antes de saber si el bloque que está ubicado en él es el que se está buscando. Esto sólo puede hacerse con operaciones de lectura, ya que si finalmente el bloque no es el adecuado, no se ha modificado sun contenido.

Pero una operación de escritura sólo puede hacerse si la comparación de etiquetas da como resultado que el bloque ubicado en ese marco es el que se estaba buscando; no se puede hacer trabajo en paralelo:

\begin{itemize}
  \item \textbf{Escritura directa:} cuando se modifica una palabra con una instrucción de almacenamiento, se realiza la escritura en el primer nivel de caché y en el siguiente nivel de la jerarquía.
  \item \textbf{Post-Escritura:} cuando se modifica una palabra sólo se hace en el primer nivel de la memoria caché. Cuando este bloque modificado sea reemplazado, se actualizará el contenido del nivel siguiente. Para no actualizar la memoria con cada reemplazamiento que se haga, suele utilizarse un bit que indica si el bloque que está en cache ha sido modificado (sucio) y por lo tanto debe actualizarse en memoria, o no (limpio).
\end{itemize}

Por último, en el caso de algunas jerarquías de memoria, para evitar la penalización extra que las escrituras implican se ignoran los fallos de escritura, especialmente en el caso de las memorias de escritura directa.

Por lo tanto suelen encontrarse dos opciones:

\begin{itemize}
  \item \textbf{Write Allocate:} los fallos de escritura se comportan como los fallos de lectura, se trae el bloque que ha provocado el fallo a la memoria caché y se realiza en función de la política utilizada. Habitual con Post-Escritura.
  \item \textbf{No-write Allocate:} cuando se produce un fallo de escritura, el bloque se modifica directamente en el siguiente nivel de la jerarquía de memoria y no se trae a la caché. Habitual con Escritura directa.
\end{itemize}